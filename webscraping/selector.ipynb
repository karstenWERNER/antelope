{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selectorlib import Extractor\n",
    "import fake_useragent\n",
    "import requests\n",
    "import json\n",
    "# from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "# https://www.zyte.com/blog/an-introduction-to-xpath-with-examples/\n",
    "# In fact, the expressions we've just seen are using XPath's abbreviated syntax. Translating //title to the full syntax we get:\n",
    "# So, // in the abbreviated syntax is short for descendant-or-self, which means the current node or any node below it in the tree.\n",
    "# /descendant-or-self::node()/child::title\n",
    "\n",
    "#//li[position() = 1]\n",
    "# (notice that positions in XPath start at 1, not 0). We can abbreviate the expression above to:\n",
    "\n",
    "# //li[1]\n",
    "\n",
    "# Expression\t        Meaning\n",
    "# //li[position()%2=0]\t        Selects the li elements at even positions.\n",
    "# //li[a]\t                    Selects the li elements that enclose an a element.\n",
    "# //li[a or h2]\t                Selects the li elements that enclose either an a or an h2 element.\n",
    "# //li[ a [ text() = \"link\" ] ]\tSelects the li elements that enclose an a element whose text is \"link\". Can also be written as //li[ a/text()=\"link\" ].\n",
    "# //li[last()]\t                Selects the last li element in the document.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Extractor by reading from the YAML file\n",
    "e = Extractor.from_yaml_file('products.yml')\n",
    "\n",
    "def scrape(url):\n",
    "\n",
    "    ua = UserAgent()\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': ua.random,\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'referer': 'https://www.amazon.com/',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "    }\n",
    "\n",
    "    # Download the page using requests\n",
    "    print(\"Downloading %s\"%url)\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # Simple check to check if page was blocked (Usually 503)\n",
    "    if r.status_code > 500:\n",
    "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
    "        else:\n",
    "            print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
    "        return None\n",
    "    # Pass the HTML of the page and create \n",
    "    return e.extract(r.text)\n",
    "# End of function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"product_urls.txt\",'r') as urllist, open('product_output.jsonl','w') as outfile:\n",
    "    for url in urllist.read().splitlines():\n",
    "        data = scrape(url) \n",
    "        if data:\n",
    "            try:\n",
    "                data['seller_link'] = 'https://www.amazon.com' + data['seller_link']\n",
    "                data['freq_bought_link'] = 'https://www.amazon.com' + data['freq_bought_link']\n",
    "                json.dump(data,outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            except:\n",
    "                json.dump(data,outfile)\n",
    "                outfile.write(\"\\n\")"
   ]
  }
 ]
}