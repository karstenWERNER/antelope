{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import selectorlib\n",
    "from selectorlib import Extractor\n",
    "import requests\n",
    "import json\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Create an Extractor by reading from the YAML file\n",
    "e = Extractor.from_yaml_file('search.yml')\n",
    "\n",
    "def scrape(url):\n",
    "    ua = UserAgent()\n",
    "\n",
    "    headers = {\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': ua.random,\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'referer': 'https://www.amazon.com/',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "    }\n",
    "\n",
    "    # Download the page using requests\n",
    "    #print(\"Downloading %s\" % url)\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # Simple check to check if page was blocked (Usually 503)\n",
    "    if r.status_code > 500:\n",
    "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "            print(\n",
    "                \"Page %s was blocked by Amazon. Please try using better proxies\\n\" % url)\n",
    "        else:\n",
    "            print(\"Page %s must have been blocked by Amazon as the status code was %d\" % (\n",
    "                url, r.status_code))\n",
    "        return None\n",
    "    # Pass the HTML of the page and create\n",
    "    return e.extract(r.text)\n",
    "\n",
    "#category = [ \"headphones\" ]\n",
    "category = [ \"processors\", \"laptops\", \"monitors\", \"mice\", \"keyboards\", \"cameras\", \"smartphones\", \"headphones\" ]\n",
    "\n",
    "for i in category:\n",
    "    with open('search_' + str(i) + '_urls.txt', 'r') as urllist, open('search_' + str(i) + '_output.jsonl', 'w') as outfile:\n",
    "        f = open(str(i) + '_urls.txt','w')\n",
    "        for url in urllist.read().splitlines():\n",
    "            data = scrape(url)\n",
    "       \n",
    "            if data:\n",
    "                try:\n",
    "                    for product in data['products']:\n",
    "                        product['url'] = 'https://www.amazon.com' + product['url']\n",
    "                        f.write(product['url'])\n",
    "                        f.write('\\n')\n",
    "\n",
    "                        json.dump(product, outfile)\n",
    "                        outfile.write(\", \\n\")               \n",
    "                except:\n",
    "                    continue\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in category:\n",
    "    def scrape(url):\n",
    "        ua = UserAgent()\n",
    "        headers = {\n",
    "            'dnt': '1',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': ua.random,\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/   signed-exchange;v=b3;q=0.9',\n",
    "            'sec-fetch-site': 'same-origin',\n",
    "            'sec-fetch-mode': 'navigate',\n",
    "            'sec-fetch-user': '?1',\n",
    "            'sec-fetch-dest': 'document',\n",
    "            'referer': 'https://www.amazon.com/',\n",
    "            'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "        }\n",
    "        r = requests.get(url, headers=headers)\n",
    "        if r.status_code > 500:\n",
    "            if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "                print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
    "            else:\n",
    "                print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
    "            return None\n",
    "        # Pass the HTML of the page and create \n",
    "        e = Extractor.from_yaml_file(str(i) + '.yml')\n",
    "        return e.extract(r.text)\n",
    "    # End of function\n",
    "    with open(str(i) + '_urls.txt','r') as urllist, open(str(i) + '_output.jsonl','w') as outfile:\n",
    "        for url in urllist.read().splitlines():\n",
    "            data = scrape(url) \n",
    "            if data:\n",
    "                try:\n",
    "                    for xxx in [ \"freq_bought_link\", \"link_to_all_reviews\", \"seller_link\" ]:\n",
    "                        data[xxx] = 'https://www.amazon.com' + data[xxx]\n",
    "                    json.dump(data,outfile)\n",
    "                    outfile.write(\"\\n\")\n",
    "\n",
    "                except:\n",
    "                    json.dump(data,outfile)\n",
    "                    outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}